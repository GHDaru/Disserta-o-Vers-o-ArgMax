@article{alsmadi2019review,
  title={Review of short-text classification},
  author={Alsmadi, Issa and Gan, Keng Hoon},
  journal={International Journal of Web Information Systems},
  year={2019},
  publisher={Emerald Publishing Limited},
  doi={10.1108/IJWIS-12-2017-0083}
}

@inproceedings{bhavani2021review,
  title={A Review of State Art of Text Classification Algorithms},
  author={Bhavani, A and Kumar, B Santhosh},
  booktitle={2021 5th International Conference on Computing Methodologies and Communication (ICCMC)},
  pages={1484--1490},
  year={2021},
  organization={IEEE}
}

@article{song2014short,
  title={Short text classification: a survey.},
  author={Song, Ge and Ye, Yunming and Du, Xiaolin and Huang, Xiaohui and Bie, Shifu},
  journal={Journal of multimedia},
  volume={9},
  number={5},
  year={2014},
  publisher={Citeseer}
}

@article{kowsari2019text,
  title={Text classification algorithms: A survey},
  author={Kowsari, Kamran and Jafari Meimandi, Kiana and Heidarysafa, Mojtaba and Mendu, Sanjana and Barnes, Laura and Brown, Donald},
  journal={Information},
  volume={10},
  number={4},
  pages={150},
  year={2019},
  publisher={MDPI}
}

@incollection{aggarwal2012survey,
  title={A survey of text classification algorithms},
  author={Aggarwal, Charu C and Zhai, ChengXiang},
  booktitle={Mining text data},
  pages={163--222},
  year={2012},
  publisher={Springer}
}

@article{korde2012text,
  title={Text classification and classifiers: A survey},
  author={Korde, Vandana and Mahender, C Namrata},
  journal={International Journal of Artificial Intelligence \& Applications},
  volume={3},
  number={2},
  pages={85},
  year={2012},
  publisher={Academy \& Industry Research Collaboration Center (AIRCC)}
}

@article{zhang2010understanding,
  title={Understanding bag-of-words model: a statistical framework},
  author={Zhang, Yin and Jin, Rong and Zhou, Zhi-Hua},
  journal={International journal of machine learning and cybernetics},
  volume={1},
  number={1},
  pages={43--52},
  year={2010},
  publisher={Springer}
}

@article{shah2020comparative,
  title={A comparative analysis of logistic regression, random forest and KNN models for text classification},
  author={Shah, Kanish and Patel, Henil and Sanghvi, Devanshi and Shah, Manan},
  journal={Augmented Human Research},
  volume={5},
  number={1},
  pages={1--16},
  year={2020},
  publisher={Springer}
}

@book{baeza2013recuperaccao,
  title={Recuperação de Informação: Conceitos e Tecnologia das Máquinas de Busca},
  author={Baeza-Yates, Ricardo and Ribeiro-Neto, Berthier},
  year={2013},
  publisher={Bookman Editora}
}

@article{pranckevivcius2017comparison,
  title={Comparison of naive bayes, random forest, decision tree, support vector machines, and logistic regression classifiers for text reviews classification},
  author={Pranckevi{\v{c}}ius, Tomas and Marcinkevi{\v{c}}ius, Virginijus},
  journal={Baltic Journal of Modern Computing},
  volume={5},
  number={2},
  pages={221},
  year={2017},
  publisher={University of Latvia}
}

@webpage{scikitlogreg,
    author = {Scikit Learn Documentation SCIKITDOC},
    title = {Logistic Regression},
    howpublished = {Tomado de \url{https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html} (10/07/2022)},
    year = 2022
}

@webpage{scikitaccuracy,
    author = {Scikit Learn Documentation Sklearn},
    title = {Logistic Regression},
    howpublished = {Tomado de \url{https://scikit-learn.org/stable/modules/model_evaluation.html#accuracy-score} (10/07/2022)},
    year = 2022
}

@webpage{scikitsvc,
    author = {Scikit Learn Documentation Sklearn},
    title = {Support Vector Classification},
    howpublished = {Tomado de \url{https://scikit-learn.org/stable/modules/svm.html#svc} (10/07/2022)},
    year = 2022
}

@misc{mikolov,
  doi = {10.48550/ARXIV.1301.3781},
  url = {https://arxiv.org/abs/1301.3781},
  author = {Mikolov, Tomas and Chen, Kai and Corrado, Greg and Dean, Jeffrey},
  keywords = {Computation and Language (cs.CL), FOS: Computer and information sciences, FOS: Computer and information sciences},
  title = {Efficient Estimation of Word Representations in Vector Space},
  publisher = {arXiv},
  year = {2013},
  copyright = {arXiv.org perpetual, non-exclusive license}
}

@article{gandomi2015137,
title = {Beyond the hype: Big data concepts, methods, and analytics},
journal = {International Journal of Information Management},
volume = {35},
number = {2},
pages = {137-144},
year = {2015},
issn = {0268-4012},
doi = {https://doi.org/10.1016/j.ijinfomgt.2014.10.007},
url = {https://www.sciencedirect.com/science/article/pii/S0268401214001066},
author = {Amir Gandomi and Murtaza Haider},
keywords = {Big data analytics, Big data definition, Unstructured data analytics, Predictive analytics},
abstract = {Size is the first, and at times, the only dimension that leaps out at the mention of big data. This paper attempts to offer a broader definition of big data that captures its other unique and defining characteristics. The rapid evolution and adoption of big data by industry has leapfrogged the discourse to popular outlets, forcing the academic press to catch up. Academic journals in numerous disciplines, which will benefit from a relevant discussion of big data, have yet to cover the topic. This paper presents a consolidated description of big data by integrating definitions from practitioners and academics. The paper's primary focus is on the analytic methods used for big data. A particular distinguishing feature of this paper is its focus on analytics related to unstructured data, which constitute 95% of big data. This paper highlights the need to develop appropriate and efficient analytical methods to leverage massive volumes of heterogeneous data in unstructured text, audio, and video formats. This paper also reinforces the need to devise new tools for predictive analytics for structured big data. The statistical methods in practice were devised to infer from sample data. The heterogeneity, noise, and the massive size of structured big data calls for developing computationally efficient algorithms that may avoid big data pitfalls, such as spurious correlation.}
}